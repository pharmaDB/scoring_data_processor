{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Matching_Top10Percent_DrugLabelsandPatent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPeuSU3LjBza"
      },
      "source": [
        "This sheets answers the question of: out of the best models, how similar are the top X% of the results.  The best models are mpnet_base_v2, roberta, and scispacy, since all three of these models have the greatest z-scores from the noise.  See Noise_to_Related_Claims_Histogram_DrugLabelsandPatent.ipynb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JDsiezRUEQY",
        "outputId": "d8c8e4b6-711b-4be8-82e5-9be1ca4b69eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "zip_path='/content/drive/MyDrive/Colab Notebooks/zip/db2file.zip'\n",
        "!cp \"{zip_path}\" .\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/requirements.txt\" .\n",
        "!unzip -q db2file.zip\n",
        "!rm db2file.zip\n",
        "!rm -r en_core_sci_lg-0.4.0.zip\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/zip/en_core_sci_lg-0.4.0.zip\" .\n",
        "!unzip -q en_core_sci_lg-0.4.0.zip\n",
        "!rm en_core_sci_lg-0.4.0.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "rm: cannot remove 'en_core_sci_lg-0.4.0.zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wmH958tXWkKh",
        "outputId": "c9730bca-bc82-422a-e05e-a8d5cacf8d96"
      },
      "source": [
        "!pip install -r '/content/requirements.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting beautifulsoup4==4.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 30kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 81kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.7MB/s \n",
            "\u001b[?25hCollecting blis==0.7.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/c1/f364687078298233696eff17305f9a54c4a27d9da03c07c0062909d550f1/blis-0.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.8MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8MB 12.3MB/s \n",
            "\u001b[?25hCollecting catalogue==2.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: certifi==2020.12.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 4)) (2020.12.5)\n",
            "Collecting chardet==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 6)) (7.1.2)\n",
            "Collecting conllu==4.4\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cymem==2.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 8)) (2.0.5)\n",
            "Collecting diff-match-patch==20200713\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/5a/9aa3b95a1d108b82fadb1eed4c3773d19069f765bd4c360a930e107138ee/diff_match_patch-20200713-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 10)) (3.0.12)\n",
            "Collecting ftfy==5.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 12)) (2.10)\n",
            "Requirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 13)) (2.11.3)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 14)) (1.0.1)\n",
            "Collecting lxml==4.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 16)) (1.1.1)\n",
            "Requirement already satisfied: murmurhash==1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 17)) (1.0.5)\n",
            "Collecting nltk==3.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 59.0MB/s \n",
            "\u001b[?25hCollecting nmslib==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/77/aebbd03a32488024d2ae2230b47a28f6fa83c887318e673fa5d3234f7772/nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5MB 271kB/s \n",
            "\u001b[?25hCollecting numpy==1.20.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 374kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging==20.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 21)) (20.9)\n",
            "Collecting pandas==1.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/51/48f3fc47c4e2144da2806dfb6629c4dd1fa3d5a143f9652b141e979a8ca9/pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9MB 51.7MB/s \n",
            "\u001b[?25hCollecting pathy==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed==3.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 24)) (3.0.5)\n",
            "Collecting psutil==5.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/da/f7efdcf012b51506938553dbe302aecc22f3f43abd5cffa8320e8e0588d5/psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 75.9MB/s \n",
            "\u001b[?25hCollecting pybind11==2.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/84/fc9dc13ee536ba5e6b8fd10ce368fea5b738fe394c3b296cde7c9b144a92/pybind11-2.6.1-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 76.2MB/s \n",
            "\u001b[?25hCollecting pydantic==1.7.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 141kB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo==3.11.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 28)) (3.11.3)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 29)) (2.4.7)\n",
            "Collecting pysbd==0.3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/0a/c99fb7d7e176f8b176ef19704a32e6a9c6aafdf19ef75a187f701fc15801/pysbd-0.3.4-py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 31)) (2.8.1)\n",
            "Collecting python-dotenv==0.16.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/7b/4a0bcac71bf24e5a63d043e53e3f56e1838a91c760638e1a0fc5338a36aa/python_dotenv-0.16.0-py2.py3-none-any.whl\n",
            "Collecting pytz==2021.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 60.7MB/s \n",
            "\u001b[?25hCollecting regex==2021.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 73.3MB/s \n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.45\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 64.6MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.3MB/s \n",
            "\u001b[?25hCollecting scipy==1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e8/43ffca541d2f208d516296950b25fe1084b35c2881f4d444c1346ca75815/scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting scispacy==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/d2/456e1f66f7ba65209746aac666b22e0c11e9aee6d9f549a2fdba5d49247b/scispacy-0.4.0-py3-none-any.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting sentence-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 42)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 43)) (0.0)\n",
            "Collecting smart-open==3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 76.6MB/s \n",
            "\u001b[?25hCollecting soupsieve==2.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
            "Collecting spacy==3.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 133kB/s \n",
            "\u001b[?25hCollecting spacy-alignments==0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/86/a6786d24d1d8f3a6cff2c60b55a7e845725a94919cd94d270ea49d82e59b/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 56.2MB/s \n",
            "\u001b[?25hCollecting spacy-legacy==3.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Collecting spacy-transformers==1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/c5/a156f9c979cc14f5f41cf2e6ecfc55d1128ac0363930ec7cc6fe4d98b4a2/spacy_transformers-1.0.2-py2.py3-none-any.whl\n",
            "Collecting srsly==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 69.8MB/s \n",
            "\u001b[?25hCollecting thinc==8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 65.9MB/s \n",
            "\u001b[?25hCollecting threadpoolctl==2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Collecting tokenizers==0.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 69.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 54)) (1.8.1+cu101)\n",
            "Collecting torchcontrib==0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/72/36/45d475035ab35353911e72a03c1c1210eba63b71e5a6917a9e78a046aa10/torchcontrib-0.0.2.tar.gz\n",
            "Collecting tqdm==4.60.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hCollecting transformers==4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 56.0MB/s \n",
            "\u001b[?25hCollecting typer==0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 59)) (3.7.4.3)\n",
            "Collecting urllib3==1.26.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 75.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi==0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 61)) (0.8.2)\n",
            "Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements.txt (line 62)) (0.2.5)\n",
            "Collecting xmltodict==0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue==2.0.4->-r /content/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6->-r /content/requirements.txt (line 46)) (56.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/requirements.txt (line 57)) (3.10.1)\n",
            "Building wheels for collected packages: ftfy, sentence-transformers, smart-open, torchcontrib\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=f02c5917e8bca545780e28e27ad837e8d58747e34bde383438361b6d77137114\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=0d4bc1e24993636f12e9a17ff308b8e4245ea66292ca9d2ea6cc464b250b2434\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=7592e9434e8e4482eafa66d3a97adef1691e03138aa2649e7aa4332d8cf74668\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-cp37-none-any.whl size=7532 sha256=c3e3d89f04a2620b79581164783b01d2431af7b1d04e0dc3100e58633058afac\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/06/7b/a5f5920bbf4f12a2c927e438fac17d4cd9560f8336b00e9a99\n",
            "Successfully built ftfy sentence-transformers smart-open torchcontrib\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: soupsieve, beautifulsoup4, numpy, blis, catalogue, chardet, conllu, diff-match-patch, ftfy, lxml, tqdm, regex, nltk, psutil, pybind11, nmslib, pytz, pandas, typer, urllib3, requests, smart-open, pathy, pydantic, pysbd, python-dotenv, sacremoses, threadpoolctl, scipy, scikit-learn, srsly, thinc, spacy-legacy, spacy, scispacy, tokenizers, transformers, sentencepiece, sentence-transformers, spacy-alignments, spacy-transformers, torchcontrib, xmltodict\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed beautifulsoup4-4.9.3 blis-0.7.4 catalogue-2.0.4 chardet-4.0.0 conllu-4.4 diff-match-patch-20200713 ftfy-5.9 lxml-4.6.3 nltk-3.6.2 nmslib-2.1.1 numpy-1.20.2 pandas-1.2.4 pathy-0.5.2 psutil-5.8.0 pybind11-2.6.1 pydantic-1.7.3 pysbd-0.3.4 python-dotenv-0.16.0 pytz-2021.1 regex-2021.4.4 requests-2.25.1 sacremoses-0.0.45 scikit-learn-0.24.2 scipy-1.6.3 scispacy-0.4.0 sentence-transformers-1.1.0 sentencepiece-0.1.95 smart-open-3.0.0 soupsieve-2.2.1 spacy-3.0.6 spacy-alignments-0.8.3 spacy-legacy-3.0.5 spacy-transformers-1.0.2 srsly-2.4.1 thinc-8.0.3 threadpoolctl-2.1.0 tokenizers-0.10.2 torchcontrib-0.0.2 tqdm-4.60.0 transformers-4.5.1 typer-0.3.2 urllib3-1.26.4 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "psutil",
                  "pytz"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAlDL4NXGWL"
      },
      "source": [
        "import random\n",
        "import os\n",
        "random.seed(30)\n",
        "db2files = \"/content/db2file/\"\n",
        "NDA_list=[f for f in os.listdir(db2files)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBDg-2lZg50N"
      },
      "source": [
        "def get_lines_in_file(file_name):\n",
        "  if os.path.exists(file_name):\n",
        "    f = open(file_name, \"rb\")\n",
        "    return_list = [str(line.decode('unicode_escape')) for line in f if line.decode('unicode_escape').strip()]\n",
        "    f.close()\n",
        "    return return_list\n",
        "  else:\n",
        "    return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqZgsrZj9SUO"
      },
      "source": [
        "def flat_list(lst):\n",
        "  return [item for sublist in lst for item in sublist]\n",
        "\n",
        "def get_additions(NDA, additions_folder_name):\n",
        "  \"\"\" \n",
        "  Return all additions as a list for the set-id with most additions for a NDA \n",
        "  excluding the first addition.\n",
        "  Parameters:\n",
        "      NDA (string): NDA dir\n",
        "      additions_folder_name (string): either 'just_additions' or 'additions_with_context'\n",
        "  \"\"\"\n",
        "  if additions_folder_name not in ['just_additions', 'additions_with_context']:\n",
        "    print(f\"Parameter {additions_folder_name} not in ['just_additions', 'additions_with_context']\")\n",
        "    return []\n",
        "  NDA_dir=db2files+str(NDA)+'/'\n",
        "  set_id_dirs=[f for f in os.listdir(NDA_dir)]\n",
        "  try:\n",
        "    set_id_dirs.remove('patents')\n",
        "  except ValueError:\n",
        "    pass\n",
        "  additions_list=[]\n",
        "  for set_id_dir in set_id_dirs:\n",
        "    additions_dir=NDA_dir+set_id_dir+'/'+additions_folder_name+'/'\n",
        "    if os.path.exists(additions_dir):\n",
        "      additions_files=sorted([additions_dir+f for f in os.listdir(additions_dir)])[1:]\n",
        "      additions_list_tmp=flat_list([get_lines_in_file(file) for file in additions_files])\n",
        "      if len(additions_list_tmp)> len(additions_list):\n",
        "        additions_list=additions_list_tmp\n",
        "  return additions_list\n",
        "\n",
        "def get_patent_claims(NDA, patents_folder_name):\n",
        "  \"\"\"Return a list of patents claims for a NDA\n",
        "  Parameters:\n",
        "      NDA (string): NDA dir\n",
        "      patents_folder_name (string): either 'patents' or 'patents_longhand'\n",
        "  \"\"\"\n",
        "  patent_dir=db2files+str(NDA)+'/'+patents_folder_name+'/'\n",
        "  if os.path.exists(patent_dir):\n",
        "    patent_files=[patent_dir+f for f in os.listdir(patent_dir)]\n",
        "    return flat_list([get_lines_in_file(file) for file in patent_files])\n",
        "  return []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH_rqHMEpbnm",
        "outputId": "6e76c006-1613-406c-b783-6d183548221d"
      },
      "source": [
        "random_NDA_list=random.sample(NDA_list, int(len(NDA_list)*.33))\n",
        "print(len(NDA_list), len(random_NDA_list), random_NDA_list[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1606 529 202895-21976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIG3xW2C5Suc"
      },
      "source": [
        "# narrow 1/3 of random data to NDA with patents and NDAs with additions.  If either is missing, we cannot check quality of additions to related patents.\n",
        "random_NDA_list=[x for x in random_NDA_list if get_patent_claims(x, 'patents') and get_additions(x, 'additions_with_context')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFuJ6ViNIAVU",
        "outputId": "ec4408c8-92db-4bd9-9ed1-9077f31806d1"
      },
      "source": [
        "print(len(random_NDA_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNvnVArssTah"
      },
      "source": [
        "def return_match(NDA, additions_folder_name, patent_folder_name, scoring_method_list, cutoff_percentage=.1):\n",
        "  \"\"\"\n",
        "  This method returns {\"method_name\":{\"additions_len\": X, \"claims_len\": X, \"matches\": [[additions_num, claim_num],] }}\n",
        "  Parameters:\n",
        "    random_NDA_list (list): list of NDA numbers\n",
        "    additions_folder_name (string): either 'patents' or 'patents_longhand'\n",
        "    patent_folder_name (string): either 'just_additions' or 'additions_with_context'\n",
        "    scoring_method_list (list): list of [[function that is use to score similarity, optional_scoring_method_field],..]\n",
        "  \"\"\"\n",
        "\n",
        "  # scoring_method_result_dict={ \"scoring_method.__name__\": {\"additions_len\": X, \"claims_len\": X, \"matches\":[(addition, claim), }}\n",
        "  scoring_method_result_dict={}\n",
        "\n",
        "  for i in range(len(scoring_method_list)):\n",
        "    scoring_method=scoring_method_list[i][0]\n",
        "    optional_scoring_method_field=scoring_method_list[i][1]\n",
        "    optional_scoring_method_field_name=scoring_method_list[i][2]\n",
        "\n",
        "    claims = get_patent_claims(NDA, patent_folder_name)\n",
        "    additions = get_additions(NDA, additions_folder_name)\n",
        "\n",
        "    matrix=scoring_method(additions, claims, optional_scoring_method_field)\n",
        "    # get top 10 percent of each row/addition of the matrix.\n",
        "    top_10 = top_10_percent_matrix(matrix, cutoff_percentage)\n",
        "    key = optional_scoring_method_field_name if optional_scoring_method_field else scoring_method.__name__\n",
        "    scoring_method_result_dict[key]={\n",
        "        \"claims_len\":len(claims),\n",
        "        \"matches\": top_10,\n",
        "    }\n",
        "  return scoring_method_result_dict\n",
        "\n",
        "import math\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1JlbMDr11Cl"
      },
      "source": [
        "def top_10_percent_matrix(matrix, cutoff_percentage):\n",
        "  \"\"\" Return a matrix: ie. [[claim_num_index, claim_num_index],] for top 10% of each match.\n",
        "        The rows represents additions.\n",
        "\n",
        "  Parameters:\n",
        "    matrix (list of list): a matrix where the rows represents additions, and the columns, claims for all additions to all claims\n",
        "  \"\"\"\n",
        "  return_matrix_of_index=[]\n",
        "  for row in matrix:\n",
        "    length_10percent=math.ceil(len(row)*cutoff_percentage)\n",
        "    # get indices of top 10 in max_scores_list\n",
        "    indexes=sorted(range(len(row)), key=lambda i: row[i], reverse=True)[:length_10percent]\n",
        "    return_matrix_of_index.append(indexes)\n",
        "  return return_matrix_of_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T621ChlOv3JN"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def scoring_method_bert(additions, claims, model):\n",
        "  \"\"\" Returns a list of [[additions_claim, claim_num],]\n",
        "  \"\"\"\n",
        "  # Compute embedding for both lists\n",
        "  additions_embeddings = model.encode(\n",
        "      additions,\n",
        "      convert_to_tensor=True,\n",
        "  )\n",
        "  claims_embeddings = model.encode(\n",
        "      claims,\n",
        "      convert_to_tensor=True,\n",
        "  )\n",
        "  # Compute cosine-similarity for every additions to every claim\n",
        "  cosine_scores = util.pytorch_cos_sim(\n",
        "      additions_embeddings, claims_embeddings\n",
        "  ).tolist()\n",
        "  return cosine_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-yGTUILaIsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b2c49a-beb4-451f-91b4-920ed2194358"
      },
      "source": [
        "import spacy\n",
        "gpu = spacy.prefer_gpu()\n",
        "print('GPU:', gpu)\n",
        "!pip install -U spacy[cuda101]\n",
        "_N_PROCESS = 1\n",
        "_en_core_sci_lg_nlp = spacy.load(\"/content/en_core_sci_lg-0.4.0/en_core_sci_lg/en_core_sci_lg-0.4.0\")\n",
        "!python -m spacy download en_core_web_trf\n",
        "nlp_en = spacy.load('en_core_web_trf')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: True\n",
            "Requirement already up-to-date: spacy[cuda101] in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (8.0.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (0.7.4)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (1.20.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (4.60.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (2.25.1)\n",
            "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (1.7.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: cupy-cuda101<9.0.0,>=5.0.0b4; extra == \"cuda101\" in /usr/local/lib/python3.7/dist-packages (from spacy[cuda101]) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[cuda101]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[cuda101]) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy[cuda101]) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]) (1.26.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[cuda101]) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy[cuda101]) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda101<9.0.0,>=5.0.0b4; extra == \"cuda101\"->spacy[cuda101]) (0.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda101<9.0.0,>=5.0.0b4; extra == \"cuda101\"->spacy[cuda101]) (1.15.0)\n",
            "2021-05-05 19:04:14.015027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Requirement already satisfied: en-core-web-trf==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl#egg=en_core_web_trf==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy-transformers<1.1.0,>=1.0.0rc4 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (1.0.2)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.8.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: transformers<4.6.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (4.5.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (56.0.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.60.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.20.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2021.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.0.45)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.26.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTyvGYJx6DNU"
      },
      "source": [
        "\n",
        "def preprocess_with_spacy_nlp(text_list, steps, nlp=nlp_en):\n",
        "    \"\"\"\n",
        "    This method can remove punctuation,\n",
        "    Parameters:\n",
        "        text_list (list): list of strings\n",
        "        steps (list): one of [\"punct\", \"lemma\", \"stopwords\"]\n",
        "    \"\"\"\n",
        "    # make a copy of text_lis\n",
        "    return_list = text_list\n",
        "    if any(item in [\"punct\", \"lemma\", \"stopwords\"] for item in steps):\n",
        "        # 'lemmatizer' required 'tagger' and 'attribute_ruler'\n",
        "        nlp_list = list(\n",
        "            nlp.pipe(\n",
        "                return_list,\n",
        "                disable=[\"tok2vec\", \"ner\"],\n",
        "                n_process=_N_PROCESS,\n",
        "            )\n",
        "        )\n",
        "        return_list = [\n",
        "            \" \".join(\n",
        "                [\n",
        "                    token.lemma_ if \"lemma\" in steps else token.text\n",
        "                    for token in doc\n",
        "                    if (\n",
        "                        (\n",
        "                            (\"punct\" in steps and not token.is_punct)\n",
        "                            or \"punct\" not in steps\n",
        "                        )\n",
        "                        and (\n",
        "                            (\"stopwords\" in steps and not token.is_stop)\n",
        "                            or \"stopwords\" not in steps\n",
        "                        )\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "            for doc in nlp_list\n",
        "        ]\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def similarity_matrix(embed_A_list, embed_B_list):\n",
        "    \"\"\"\n",
        "    This method returns a matrix such as:\n",
        "        [[X, X, X],\n",
        "        [X, X, X]]\n",
        "    wherein each row represents the similarity measurement between an embedding\n",
        "    from embed_A_list to each of the embeddings in embed_B_list.\n",
        "\n",
        "    Parameters:\n",
        "        embed_A_list (list): list of NLP object generated by spaCy\n",
        "        embed_B_list (list): list of NLP object generated by spaCy to be\n",
        "                             compared to embed_A\n",
        "    \"\"\"\n",
        "    matrix = [[0] * len(embed_B_list) for y in range(len(embed_A_list))]\n",
        "    for i in range(len(embed_A_list)):\n",
        "        for j in range(len(embed_B_list)):\n",
        "            matrix[i][j] = embed_A_list[i].similarity(embed_B_list[j])\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def scoring_method_spacy(additions, claims, nlp=nlp_en):\n",
        "  \"\"\" Scores with spaCy; return a matrix of similar scores\n",
        "  \"\"\"\n",
        "  additions=preprocess_with_spacy_nlp(additions, [\"punct\", \"lemma\", \"stopwords\"], nlp)\n",
        "  claims=preprocess_with_spacy_nlp(claims, [\"punct\", \"lemma\", \"stopwords\"], nlp)\n",
        "  # Compute embedding for both lists\n",
        "  # tokenization only requires tok2vec\n",
        "  disabled_list = [\"tagger\", \"attribute_ruler\", \"lemmatizer\", \"parser\", \"ner\"]\n",
        "  additions_embeddings = list(\n",
        "      nlp.pipe(\n",
        "          additions, disable=disabled_list, n_process=_N_PROCESS\n",
        "      )\n",
        "  )\n",
        "  claims_embeddings = list(\n",
        "      nlp.pipe(\n",
        "          claims, disable=disabled_list, n_process=_N_PROCESS\n",
        "      )\n",
        "  )\n",
        "  # Compute cosine-similarity for every additions to every claim\n",
        "  return similarity_matrix(additions_embeddings, claims_embeddings)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Aj-QY0EklL"
      },
      "source": [
        "_device = None\n",
        "\n",
        "model_mpnet_base_v2 = SentenceTransformer(\"stsb-mpnet-base-v2\", device=_device)\n",
        "model_mpnet_base_v2.zero_grad()\n",
        "# Will limit size since CUDA runs out of memory\n",
        "model_mpnet_base_v2.max_seq_length=512\n",
        "\n",
        "model_roberta_base_v2 = SentenceTransformer(\"stsb-roberta-base-v2\", device=_device)\n",
        "model_roberta_base_v2.zero_grad()\n",
        "# Will limit size since CUDA runs out of memory\n",
        "model_roberta_base_v2.max_seq_length=512\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C9qpkiKXcwS",
        "outputId": "6cdcb301-3702-4b81-e353-ba1e556c7d4e"
      },
      "source": [
        "\n",
        "scoring_method_list=[\n",
        "                     [scoring_method_bert, model_mpnet_base_v2, \"model_mpnet_base_v2\"],\n",
        "                     [scoring_method_bert,model_roberta_base_v2, \"model_roberta_base_v2\"],\n",
        "                     [scoring_method_spacy,nlp_en, \"en_core_web_trf\"],\n",
        "                     [scoring_method_spacy,_en_core_sci_lg_nlp, \"_en_core_sci_lg_nlp\"],]\n",
        "\n",
        "NDA=random_NDA_list[3]\n",
        "\n",
        "# return_match_result={\"method_name\":{\"claims_len\": X, \"matches\": [[claim_index, claim_index],] }}, where matches are index/specific line in the patent file when all files are joined together\n",
        "return_match_result=return_match(NDA, \"additions_with_context\", \"patents\", scoring_method_list, cutoff_percentage=.1)\n",
        "print(return_match_result)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'model_mpnet_base_v2': {'claims_len': 176, 'matches': [[101, 112, 109, 164, 22, 142, 39, 51, 0, 114, 118, 110, 103, 175, 113, 115, 111, 159], [92, 78, 68, 65, 154, 79, 97, 64, 34, 91, 60, 70, 49, 81, 61, 87, 82, 50], [101, 112, 109, 142, 22, 164, 39, 51, 0, 114, 118, 115, 110, 175, 111, 103, 113, 159], [131, 91, 61, 65, 96, 86, 76, 31, 81, 23, 62, 119, 78, 35, 27, 82, 60, 85], [128, 129, 170, 130, 42, 138, 168, 167, 86, 87, 139, 26, 165, 140, 23, 35, 27, 55], [101, 109, 112, 22, 142, 39, 0, 164, 51, 110, 111, 114, 115, 175, 118, 75, 159, 73], [142, 0, 22, 109, 101, 39, 112, 164, 51, 175, 111, 114, 2, 144, 159, 75, 162, 143], [0, 142, 101, 112, 39, 109, 51, 22, 164, 175, 111, 114, 143, 75, 110, 35, 96, 159]]}, 'model_roberta_base_v2': {'claims_len': 176, 'matches': [[101, 109, 112, 142, 118, 115, 0, 103, 22, 39, 51, 164, 75, 175, 117, 143, 59, 1], [87, 97, 92, 54, 82, 62, 84, 94, 52, 51, 99, 30, 55, 38, 59, 40, 53, 89], [101, 109, 142, 112, 118, 115, 0, 22, 103, 39, 164, 117, 51, 75, 143, 175, 116, 1], [119, 65, 87, 64, 82, 131, 92, 97, 174, 171, 170, 172, 78, 173, 79, 166, 165, 167], [128, 138, 140, 139, 129, 130, 73, 97, 127, 55, 119, 43, 84, 38, 26, 82, 32, 92], [101, 109, 142, 112, 0, 118, 115, 22, 39, 103, 51, 75, 164, 47, 59, 175, 117, 143], [142, 0, 101, 22, 109, 115, 118, 112, 39, 164, 51, 175, 75, 143, 117, 1, 103, 59], [142, 0, 101, 22, 39, 115, 109, 51, 118, 112, 175, 164, 143, 75, 117, 1, 103, 47]]}, 'en_core_web_trf': {'claims_len': 176, 'matches': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]]}, '_en_core_sci_lg_nlp': {'claims_len': 176, 'matches': [[101, 112, 109, 110, 142, 117, 103, 113, 115, 0, 114, 143, 1, 57, 118, 45, 22, 58], [119, 65, 79, 78, 131, 63, 77, 68, 122, 120, 73, 27, 23, 31, 133, 35, 64, 81], [101, 112, 109, 117, 142, 115, 118, 0, 1, 143, 110, 75, 103, 45, 57, 46, 114, 58], [64, 123, 121, 65, 119, 79, 131, 69, 138, 128, 97, 31, 23, 92, 87, 27, 78, 82], [76, 138, 128, 64, 131, 119, 79, 65, 123, 69, 121, 78, 35, 31, 32, 23, 36, 24], [101, 112, 142, 109, 117, 115, 0, 118, 110, 1, 143, 75, 103, 45, 57, 114, 22, 113], [101, 142, 112, 0, 109, 110, 103, 143, 1, 114, 111, 113, 117, 57, 115, 56, 44, 45], [101, 142, 112, 0, 109, 110, 1, 143, 103, 111, 114, 113, 56, 44, 57, 117, 115, 45]]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpVmh_EkVy-",
        "outputId": "c917c28d-8733-4253-80c8-88135870ddd4"
      },
      "source": [
        "# Remove en_core_web_trf, since results indicated that there is no vectors\n",
        "scoring_method_list=[\n",
        "                     [scoring_method_bert, model_mpnet_base_v2, \"model_mpnet_base_v2\"],\n",
        "                     [scoring_method_bert,model_roberta_base_v2, \"model_roberta_base_v2\"],\n",
        "                     [scoring_method_spacy,_en_core_sci_lg_nlp, \"_en_core_sci_lg_nlp\"],]\n",
        "\n",
        "multi_return_match_result=[]\n",
        "for i in range(len(random_NDA_list)):\n",
        "  print(f\"processing {str(i)} of {str(len(random_NDA_list))}\")\n",
        "  NDA=random_NDA_list[i]\n",
        "  multi_return_match_result.append(return_match(NDA, \"additions_with_context\", \"patents\", scoring_method_list, cutoff_percentage=.1))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing 0 of 292\n",
            "processing 1 of 292\n",
            "processing 2 of 292\n",
            "processing 3 of 292\n",
            "processing 4 of 292\n",
            "processing 5 of 292\n",
            "processing 6 of 292\n",
            "processing 7 of 292\n",
            "processing 8 of 292\n",
            "processing 9 of 292\n",
            "processing 10 of 292\n",
            "processing 11 of 292\n",
            "processing 12 of 292\n",
            "processing 13 of 292\n",
            "processing 14 of 292\n",
            "processing 15 of 292\n",
            "processing 16 of 292\n",
            "processing 17 of 292\n",
            "processing 18 of 292\n",
            "processing 19 of 292\n",
            "processing 20 of 292\n",
            "processing 21 of 292\n",
            "processing 22 of 292\n",
            "processing 23 of 292\n",
            "processing 24 of 292\n",
            "processing 25 of 292\n",
            "processing 26 of 292\n",
            "processing 27 of 292\n",
            "processing 28 of 292\n",
            "processing 29 of 292\n",
            "processing 30 of 292\n",
            "processing 31 of 292\n",
            "processing 32 of 292\n",
            "processing 33 of 292\n",
            "processing 34 of 292\n",
            "processing 35 of 292\n",
            "processing 36 of 292\n",
            "processing 37 of 292\n",
            "processing 38 of 292\n",
            "processing 39 of 292\n",
            "processing 40 of 292\n",
            "processing 41 of 292\n",
            "processing 42 of 292\n",
            "processing 43 of 292\n",
            "processing 44 of 292\n",
            "processing 45 of 292\n",
            "processing 46 of 292\n",
            "processing 47 of 292\n",
            "processing 48 of 292\n",
            "processing 49 of 292\n",
            "processing 50 of 292\n",
            "processing 51 of 292\n",
            "processing 52 of 292\n",
            "processing 53 of 292\n",
            "processing 54 of 292\n",
            "processing 55 of 292\n",
            "processing 56 of 292\n",
            "processing 57 of 292\n",
            "processing 58 of 292\n",
            "processing 59 of 292\n",
            "processing 60 of 292\n",
            "processing 61 of 292\n",
            "processing 62 of 292\n",
            "processing 63 of 292\n",
            "processing 64 of 292\n",
            "processing 65 of 292\n",
            "processing 66 of 292\n",
            "processing 67 of 292\n",
            "processing 68 of 292\n",
            "processing 69 of 292\n",
            "processing 70 of 292\n",
            "processing 71 of 292\n",
            "processing 72 of 292\n",
            "processing 73 of 292\n",
            "processing 74 of 292\n",
            "processing 75 of 292\n",
            "processing 76 of 292\n",
            "processing 77 of 292\n",
            "processing 78 of 292\n",
            "processing 79 of 292\n",
            "processing 80 of 292\n",
            "processing 81 of 292\n",
            "processing 82 of 292\n",
            "processing 83 of 292\n",
            "processing 84 of 292\n",
            "processing 85 of 292\n",
            "processing 86 of 292\n",
            "processing 87 of 292\n",
            "processing 88 of 292\n",
            "processing 89 of 292\n",
            "processing 90 of 292\n",
            "processing 91 of 292\n",
            "processing 92 of 292\n",
            "processing 93 of 292\n",
            "processing 94 of 292\n",
            "processing 95 of 292\n",
            "processing 96 of 292\n",
            "processing 97 of 292\n",
            "processing 98 of 292\n",
            "processing 99 of 292\n",
            "processing 100 of 292\n",
            "processing 101 of 292\n",
            "processing 102 of 292\n",
            "processing 103 of 292\n",
            "processing 104 of 292\n",
            "processing 105 of 292\n",
            "processing 106 of 292\n",
            "processing 107 of 292\n",
            "processing 108 of 292\n",
            "processing 109 of 292\n",
            "processing 110 of 292\n",
            "processing 111 of 292\n",
            "processing 112 of 292\n",
            "processing 113 of 292\n",
            "processing 114 of 292\n",
            "processing 115 of 292\n",
            "processing 116 of 292\n",
            "processing 117 of 292\n",
            "processing 118 of 292\n",
            "processing 119 of 292\n",
            "processing 120 of 292\n",
            "processing 121 of 292\n",
            "processing 122 of 292\n",
            "processing 123 of 292\n",
            "processing 124 of 292\n",
            "processing 125 of 292\n",
            "processing 126 of 292\n",
            "processing 127 of 292\n",
            "processing 128 of 292\n",
            "processing 129 of 292\n",
            "processing 130 of 292\n",
            "processing 131 of 292\n",
            "processing 132 of 292\n",
            "processing 133 of 292\n",
            "processing 134 of 292\n",
            "processing 135 of 292\n",
            "processing 136 of 292\n",
            "processing 137 of 292\n",
            "processing 138 of 292\n",
            "processing 139 of 292\n",
            "processing 140 of 292\n",
            "processing 141 of 292\n",
            "processing 142 of 292\n",
            "processing 143 of 292\n",
            "processing 144 of 292\n",
            "processing 145 of 292\n",
            "processing 146 of 292\n",
            "processing 147 of 292\n",
            "processing 148 of 292\n",
            "processing 149 of 292\n",
            "processing 150 of 292\n",
            "processing 151 of 292\n",
            "processing 152 of 292\n",
            "processing 153 of 292\n",
            "processing 154 of 292\n",
            "processing 155 of 292\n",
            "processing 156 of 292\n",
            "processing 157 of 292\n",
            "processing 158 of 292\n",
            "processing 159 of 292\n",
            "processing 160 of 292\n",
            "processing 161 of 292\n",
            "processing 162 of 292\n",
            "processing 163 of 292\n",
            "processing 164 of 292\n",
            "processing 165 of 292\n",
            "processing 166 of 292\n",
            "processing 167 of 292\n",
            "processing 168 of 292\n",
            "processing 169 of 292\n",
            "processing 170 of 292\n",
            "processing 171 of 292\n",
            "processing 172 of 292\n",
            "processing 173 of 292\n",
            "processing 174 of 292\n",
            "processing 175 of 292\n",
            "processing 176 of 292\n",
            "processing 177 of 292\n",
            "processing 178 of 292\n",
            "processing 179 of 292\n",
            "processing 180 of 292\n",
            "processing 181 of 292\n",
            "processing 182 of 292\n",
            "processing 183 of 292\n",
            "processing 184 of 292\n",
            "processing 185 of 292\n",
            "processing 186 of 292\n",
            "processing 187 of 292\n",
            "processing 188 of 292\n",
            "processing 189 of 292\n",
            "processing 190 of 292\n",
            "processing 191 of 292\n",
            "processing 192 of 292\n",
            "processing 193 of 292\n",
            "processing 194 of 292\n",
            "processing 195 of 292\n",
            "processing 196 of 292\n",
            "processing 197 of 292\n",
            "processing 198 of 292\n",
            "processing 199 of 292\n",
            "processing 200 of 292\n",
            "processing 201 of 292\n",
            "processing 202 of 292\n",
            "processing 203 of 292\n",
            "processing 204 of 292\n",
            "processing 205 of 292\n",
            "processing 206 of 292\n",
            "processing 207 of 292\n",
            "processing 208 of 292\n",
            "processing 209 of 292\n",
            "processing 210 of 292\n",
            "processing 211 of 292\n",
            "processing 212 of 292\n",
            "processing 213 of 292\n",
            "processing 214 of 292\n",
            "processing 215 of 292\n",
            "processing 216 of 292\n",
            "processing 217 of 292\n",
            "processing 218 of 292\n",
            "processing 219 of 292\n",
            "processing 220 of 292\n",
            "processing 221 of 292\n",
            "processing 222 of 292\n",
            "processing 223 of 292\n",
            "processing 224 of 292\n",
            "processing 225 of 292\n",
            "processing 226 of 292\n",
            "processing 227 of 292\n",
            "processing 228 of 292\n",
            "processing 229 of 292\n",
            "processing 230 of 292\n",
            "processing 231 of 292\n",
            "processing 232 of 292\n",
            "processing 233 of 292\n",
            "processing 234 of 292\n",
            "processing 235 of 292\n",
            "processing 236 of 292\n",
            "processing 237 of 292\n",
            "processing 238 of 292\n",
            "processing 239 of 292\n",
            "processing 240 of 292\n",
            "processing 241 of 292\n",
            "processing 242 of 292\n",
            "processing 243 of 292\n",
            "processing 244 of 292\n",
            "processing 245 of 292\n",
            "processing 246 of 292\n",
            "processing 247 of 292\n",
            "processing 248 of 292\n",
            "processing 249 of 292\n",
            "processing 250 of 292\n",
            "processing 251 of 292\n",
            "processing 252 of 292\n",
            "processing 253 of 292\n",
            "processing 254 of 292\n",
            "processing 255 of 292\n",
            "processing 256 of 292\n",
            "processing 257 of 292\n",
            "processing 258 of 292\n",
            "processing 259 of 292\n",
            "processing 260 of 292\n",
            "processing 261 of 292\n",
            "processing 262 of 292\n",
            "processing 263 of 292\n",
            "processing 264 of 292\n",
            "processing 265 of 292\n",
            "processing 266 of 292\n",
            "processing 267 of 292\n",
            "processing 268 of 292\n",
            "processing 269 of 292\n",
            "processing 270 of 292\n",
            "processing 271 of 292\n",
            "processing 272 of 292\n",
            "processing 273 of 292\n",
            "processing 274 of 292\n",
            "processing 275 of 292\n",
            "processing 276 of 292\n",
            "processing 277 of 292\n",
            "processing 278 of 292\n",
            "processing 279 of 292\n",
            "processing 280 of 292\n",
            "processing 281 of 292\n",
            "processing 282 of 292\n",
            "processing 283 of 292\n",
            "processing 284 of 292\n",
            "processing 285 of 292\n",
            "processing 286 of 292\n",
            "processing 287 of 292\n",
            "processing 288 of 292\n",
            "processing 289 of 292\n",
            "processing 290 of 292\n",
            "processing 291 of 292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7fmhyKbpFFx",
        "outputId": "148c01c8-bc78-44f1-92cc-b645d6e709e6"
      },
      "source": [
        "print(multi_return_match_result[0])\n",
        "import json\n",
        "json=json.dumps(multi_return_match_result)\n",
        "f=open(\"multi_match_result.json\", \"w\")\n",
        "f.write(json)\n",
        "f.close()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'model_mpnet_base_v2': {'claims_len': 121, 'matches': [[58, 31, 71, 63, 59, 56, 32, 50, 52, 54, 55, 15, 67], [58, 31, 63, 53, 56, 71, 55, 32, 51, 59, 50, 54, 60], [63, 58, 53, 51, 0, 31, 56, 54, 15, 33, 60, 71, 52], [58, 53, 51, 31, 56, 54, 71, 52, 59, 55, 32, 63, 50], [84, 2, 119, 75, 105, 114, 104, 117, 116, 3, 115, 15, 46], [84, 2, 114, 104, 111, 119, 102, 116, 107, 110, 117, 81, 115], [58, 31, 71, 63, 53, 32, 67, 15, 59, 51, 60, 0, 56], [30, 28, 25, 95, 58, 31, 27, 0, 63, 53, 26, 91, 108], [74, 72, 63, 20, 104, 102, 33, 15, 92, 60, 58, 51, 57], [49, 104, 5, 102, 111, 92, 74, 84, 116, 43, 72, 80, 34], [117, 104, 116, 49, 119, 111, 74, 102, 39, 92, 15, 114, 33], [104, 102, 15, 33, 119, 4, 74, 114, 31, 60, 63, 0, 58], [104, 102, 107, 58, 114, 15, 74, 110, 33, 31, 119, 113, 120], [20, 57, 60, 19, 92, 91, 39, 75, 105, 58, 79, 109, 50], [74, 72, 104, 102, 15, 33, 0, 107, 120, 90, 63, 103, 20], [104, 102, 58, 15, 110, 112, 74, 72, 33, 31, 107, 63, 87], [58, 31, 91, 92, 63, 51, 60, 90, 87, 53, 15, 0, 108], [72, 74, 102, 104, 90, 20, 63, 103, 112, 15, 87, 4, 33], [104, 102, 74, 81, 87, 72, 120, 80, 20, 4, 79, 109, 15], [104, 76, 106, 81, 102, 4, 5, 120, 49, 58, 74, 80, 0], [104, 120, 102, 87, 74, 107, 1, 72, 79, 109, 80, 0, 108], [30, 95, 28, 25, 27, 58, 75, 105, 63, 53, 0, 31, 26], [95, 30, 0, 60, 63, 31, 58, 91, 15, 33, 28, 57, 75], [91, 63, 58, 31, 0, 60, 33, 95, 15, 57, 35, 108, 19], [104, 102, 107, 58, 114, 15, 74, 110, 33, 31, 119, 113, 120], [104, 102, 58, 15, 110, 112, 74, 72, 33, 31, 107, 63, 87], [5, 81, 76, 106, 104, 102, 120, 49, 77, 6, 40, 4, 43], [58, 31, 63, 71, 0, 32, 80, 15, 59, 50, 56, 33, 60], [58, 63, 95, 0, 31, 30, 60, 80, 15, 17, 33, 91, 51], [104, 74, 102, 33, 15, 107, 72, 0, 90, 120, 103, 1, 4], [58, 31, 63, 71, 53, 51, 55, 60, 15, 32, 56, 92, 33], [102, 104, 119, 92, 117, 114, 120, 75, 105, 74, 93, 107, 116], [74, 72, 43, 20, 21, 4, 19, 104, 116, 102, 55, 107, 99], [20, 57, 60, 92, 91, 19, 75, 105, 39, 79, 109, 63, 90], [74, 72, 90, 63, 102, 104, 33, 15, 0, 117, 103, 114, 112], [74, 72, 104, 102, 15, 33, 0, 107, 120, 90, 63, 103, 20], [49, 92, 58, 55, 31, 50, 63, 111, 51, 81, 60, 9, 5], [49, 72, 74, 5, 92, 11, 102, 6, 104, 9, 63, 40, 7], [49, 92, 55, 58, 31, 20, 63, 39, 51, 75, 105, 53, 50], [72, 74, 117, 102, 92, 49, 20, 104, 39, 116, 33, 15, 119], [58, 31, 92, 91, 108, 78, 63, 55, 107, 56, 53, 60, 51], [72, 74, 107, 102, 104, 103, 120, 114, 112, 90, 33, 92, 4], [58, 31, 91, 92, 63, 51, 60, 90, 87, 53, 15, 0, 108], [104, 74, 102, 33, 15, 107, 72, 0, 120, 90, 103, 1, 4], [102, 104, 74, 72, 120, 4, 20, 33, 15, 67, 21, 0, 65], [74, 72, 90, 102, 104, 33, 63, 112, 117, 0, 15, 114, 92], [49, 72, 74, 5, 92, 6, 102, 11, 104, 43, 7, 9, 111], [74, 72, 117, 92, 102, 104, 49, 116, 20, 33, 119, 39, 15], [72, 74, 107, 102, 104, 103, 120, 114, 90, 112, 92, 33, 4], [72, 74, 102, 104, 90, 87, 112, 20, 103, 4, 15, 33, 0], [4, 67, 104, 0, 11, 20, 31, 102, 15, 58, 120, 33, 21], [4, 67, 11, 20, 0, 104, 21, 102, 15, 31, 5, 1, 33]]}, 'model_roberta_base_v2': {'claims_len': 121, 'matches': [[58, 31, 55, 71, 49, 56, 63, 59, 51, 60, 32, 53, 50], [58, 31, 55, 51, 49, 53, 56, 52, 32, 54, 71, 60, 63], [91, 58, 51, 31, 63, 55, 95, 49, 53, 60, 52, 33, 71], [58, 31, 51, 53, 55, 49, 54, 60, 52, 56, 32, 63, 59], [37, 44, 43, 73, 45, 84, 62, 75, 105, 56, 38, 115, 3], [44, 45, 73, 37, 77, 81, 84, 62, 108, 78, 98, 35, 5], [31, 58, 49, 55, 51, 32, 71, 53, 63, 52, 60, 59, 54], [30, 17, 18, 95, 28, 91, 31, 63, 60, 59, 58, 51, 53], [72, 74, 102, 104, 20, 87, 55, 21, 58, 4, 33, 63, 67], [55, 74, 56, 72, 49, 54, 58, 59, 52, 104, 102, 38, 27], [55, 56, 72, 74, 102, 58, 104, 53, 38, 49, 51, 59, 84], [58, 53, 56, 55, 60, 87, 31, 59, 49, 4, 54, 72, 37], [56, 55, 72, 84, 58, 81, 53, 77, 74, 62, 87, 99, 51], [91, 72, 74, 92, 58, 99, 4, 33, 102, 104, 20, 87, 49], [74, 72, 104, 102, 33, 15, 20, 87, 0, 21, 4, 120, 117], [72, 58, 74, 55, 87, 31, 59, 56, 32, 49, 37, 51, 84], [58, 91, 31, 51, 53, 55, 49, 87, 71, 59, 92, 56, 84], [72, 74, 87, 58, 55, 104, 102, 31, 119, 115, 59, 32, 117], [102, 120, 87, 104, 84, 72, 4, 74, 115, 99, 81, 116, 119], [62, 86, 53, 58, 76, 106, 81, 56, 43, 99, 54, 84, 83], [87, 120, 74, 104, 102, 72, 4, 67, 11, 90, 115, 116, 117], [30, 28, 95, 91, 17, 63, 18, 31, 60, 58, 53, 51, 19], [91, 28, 30, 31, 51, 17, 63, 18, 53, 55, 49, 60, 58], [91, 51, 31, 55, 28, 58, 49, 53, 19, 52, 63, 17, 32], [56, 55, 72, 84, 58, 81, 53, 77, 74, 62, 87, 99, 51], [72, 58, 74, 55, 87, 31, 59, 56, 32, 49, 37, 51, 84], [62, 86, 76, 106, 108, 43, 99, 83, 78, 81, 84, 66, 87], [58, 31, 55, 71, 49, 63, 56, 60, 51, 59, 53, 50, 52], [58, 31, 52, 51, 59, 95, 63, 55, 30, 17, 56, 60, 49], [74, 15, 104, 33, 20, 102, 0, 72, 120, 4, 21, 87, 1], [58, 31, 55, 49, 53, 51, 63, 32, 71, 56, 60, 59, 52], [115, 114, 119, 117, 116, 84, 104, 102, 118, 112, 111, 74, 110], [72, 74, 38, 102, 104, 107, 116, 119, 117, 21, 115, 15, 118], [91, 92, 72, 74, 99, 4, 33, 58, 87, 102, 104, 20, 89], [72, 74, 87, 102, 63, 104, 112, 58, 55, 20, 71, 33, 21], [74, 72, 104, 102, 33, 15, 20, 87, 0, 21, 4, 120, 117], [91, 55, 58, 49, 53, 98, 31, 102, 4, 64, 51, 84, 8], [72, 74, 55, 104, 102, 49, 56, 52, 54, 32, 58, 38, 28], [55, 53, 58, 92, 91, 51, 49, 102, 4, 56, 84, 31, 67], [72, 74, 55, 102, 104, 56, 38, 53, 58, 120, 51, 116, 115], [91, 81, 58, 62, 53, 51, 99, 84, 55, 92, 108, 4, 49], [72, 74, 102, 77, 62, 104, 55, 120, 51, 107, 87, 99, 58], [58, 91, 31, 51, 53, 55, 49, 87, 71, 59, 92, 56, 84], [74, 15, 104, 33, 20, 102, 0, 72, 120, 4, 21, 87, 1], [72, 74, 102, 120, 104, 4, 87, 33, 11, 15, 67, 20, 21], [72, 74, 87, 63, 104, 102, 112, 71, 58, 55, 20, 33, 60], [74, 72, 55, 104, 49, 102, 56, 58, 52, 54, 28, 32, 31], [72, 55, 74, 104, 102, 56, 53, 38, 58, 51, 116, 63, 49], [72, 74, 102, 62, 104, 77, 55, 120, 51, 99, 58, 87, 107], [72, 74, 87, 58, 104, 55, 31, 102, 59, 119, 115, 117, 32], [71, 55, 49, 28, 56, 19, 82, 16, 96, 38, 8, 17, 108], [71, 38, 55, 16, 56, 19, 17, 28, 96, 49, 58, 59, 37]]}, '_en_core_sci_lg_nlp': {'claims_len': 121, 'matches': [[19, 28, 60, 31, 51, 58, 53, 55, 49, 50, 52, 32, 71], [54, 52, 56, 63, 32, 50, 53, 51, 55, 71, 49, 28, 31], [63, 56, 52, 54, 32, 50, 71, 59, 94, 60, 51, 53, 55], [52, 56, 54, 32, 50, 71, 59, 63, 60, 51, 53, 57, 55], [49, 58, 31, 55, 53, 51, 19, 92, 28, 111, 50, 90, 59], [49, 58, 55, 31, 53, 51, 92, 19, 28, 111, 50, 90, 114], [54, 52, 56, 32, 50, 63, 53, 51, 55, 71, 19, 28, 49], [59, 71, 51, 58, 53, 31, 52, 55, 56, 54, 28, 32, 63], [49, 58, 31, 55, 19, 53, 51, 28, 63, 111, 50, 60, 114], [49, 55, 31, 51, 58, 53, 19, 28, 50, 54, 56, 32, 52], [49, 31, 58, 55, 51, 53, 19, 28, 50, 71, 63, 59, 32], [49, 58, 31, 55, 53, 51, 19, 28, 50, 63, 71, 59, 32], [49, 58, 31, 55, 53, 51, 19, 28, 50, 63, 71, 59, 32], [91, 111, 92, 104, 112, 113, 114, 110, 90, 116, 117, 76, 106], [90, 74, 104, 111, 72, 102, 114, 112, 113, 110, 107, 89, 77], [49, 58, 31, 55, 53, 51, 19, 28, 63, 50, 71, 59, 92], [49, 58, 55, 31, 53, 91, 51, 92, 19, 28, 63, 111, 59], [49, 31, 58, 55, 51, 53, 19, 28, 50, 71, 63, 59, 32], [107, 104, 74, 111, 77, 120, 90, 114, 112, 76, 106, 113, 102], [111, 120, 112, 104, 114, 113, 116, 90, 92, 110, 88, 107, 74], [107, 104, 74, 77, 120, 76, 106, 111, 102, 79, 109, 114, 112], [51, 28, 53, 31, 55, 58, 19, 63, 52, 49, 56, 54, 32], [51, 52, 53, 56, 55, 54, 31, 28, 32, 58, 19, 49, 71], [52, 56, 54, 51, 53, 32, 55, 31, 28, 58, 71, 49, 19], [49, 58, 31, 55, 53, 51, 19, 28, 50, 63, 71, 59, 32], [49, 58, 31, 55, 53, 51, 19, 28, 63, 50, 71, 59, 92], [104, 120, 111, 107, 112, 114, 113, 92, 116, 74, 110, 90, 88], [19, 28, 60, 31, 58, 51, 53, 49, 55, 50, 71, 32, 52], [28, 58, 59, 31, 19, 71, 51, 53, 63, 55, 49, 32, 52], [90, 74, 104, 72, 102, 111, 114, 112, 107, 113, 110, 77, 89], [60, 52, 54, 56, 32, 19, 28, 50, 63, 71, 51, 53, 55], [92, 111, 49, 90, 114, 113, 112, 91, 58, 55, 31, 110, 53], [111, 112, 114, 113, 72, 74, 110, 104, 102, 116, 92, 117, 49], [91, 92, 111, 112, 104, 113, 114, 110, 90, 116, 117, 107, 74], [49, 58, 31, 55, 51, 53, 19, 28, 50, 63, 71, 60, 59], [90, 74, 104, 111, 72, 102, 114, 112, 113, 110, 107, 89, 77], [49, 55, 53, 31, 51, 91, 58, 92, 19, 50, 28, 56, 63], [49, 55, 31, 51, 53, 58, 19, 28, 50, 54, 56, 32, 52], [49, 55, 58, 31, 53, 51, 91, 92, 19, 50, 28, 63, 59], [49, 31, 58, 55, 51, 53, 19, 28, 50, 71, 63, 59, 92], [49, 55, 58, 31, 53, 51, 91, 92, 19, 28, 63, 50, 59], [49, 31, 58, 55, 51, 53, 19, 28, 50, 63, 71, 59, 32], [49, 58, 55, 31, 53, 91, 51, 92, 19, 28, 63, 111, 59], [90, 74, 104, 72, 102, 111, 114, 112, 107, 113, 110, 77, 89], [31, 49, 58, 55, 51, 53, 19, 28, 63, 60, 71, 50, 32], [49, 58, 31, 55, 51, 53, 19, 28, 63, 50, 71, 60, 59], [49, 55, 31, 51, 53, 58, 19, 28, 50, 54, 56, 52, 32], [49, 58, 31, 55, 51, 53, 19, 28, 50, 71, 63, 59, 32], [49, 31, 58, 55, 51, 53, 19, 28, 50, 63, 71, 59, 32], [49, 31, 58, 55, 51, 53, 19, 28, 50, 63, 71, 59, 32], [31, 19, 58, 28, 51, 53, 49, 55, 60, 63, 71, 32, 59], [19, 28, 31, 58, 51, 53, 55, 49, 60, 63, 71, 57, 32]]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Gn4JM7yYkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530ed2eb-36d3-484a-af96-ec18a9376370"
      },
      "source": [
        "def mean(lst):\n",
        "  return sum(lst)/len(lst)\n",
        "\n",
        "def return_mean_percent_same_for_each_nda(multi_return_match_result):\n",
        "  # calculate percentage that match\n",
        "  # percent_same_for_each_nda is a list of percentages of claims that are the same for each addtion for each NDA\n",
        "  percent_same_for_each_nda=[]\n",
        "  for NDA in multi_return_match_result:\n",
        "    # matches = [[[claim_index1,claim_index2,],],] wherein the outermost list represents list for each model, the second outermost list represented the additions\n",
        "    matches=[]\n",
        "    model_names=[]\n",
        "    # breakup dict into two lists\n",
        "    for model_name, value in NDA.items():\n",
        "      model_names.append(model_name)\n",
        "      matches.append(value['matches'])\n",
        "    if not matches or not matches[0] or not matches[0][0]:\n",
        "      continue\n",
        "\n",
        "    models_len=len(matches)\n",
        "    additions_len=len(matches[0])\n",
        "    claims_len=len(matches[0][0])\n",
        "\n",
        "    # additions_percent_same is a list of the percentage of additions that have same claim_indexes across multiple models for each addition in an NDA\n",
        "    additions_percent_same=[]\n",
        "    for a in range(additions_len):\n",
        "      # addition_matches=[[claim_index1,claim_index2,],] wherein each row represents a different model\n",
        "      addition_matches=[]\n",
        "      for m in range(models_len):\n",
        "        addition_matches.append(matches[m][a])\n",
        "      # count number of claim_index that is the same across multiple models for same addition\n",
        "      same_claims=addition_matches[0]\n",
        "      same_claims_orig_len=len(same_claims)\n",
        "      assert same_claims_orig_len==claims_len\n",
        "      for i in range(1,len(addition_matches)):\n",
        "        same_claims=list(set(same_claims).intersection(addition_matches[i]))\n",
        "      percent_same=len(same_claims)/same_claims_orig_len\n",
        "      additions_percent_same.append(percent_same)\n",
        "    \n",
        "    percent_same_for_each_nda.append(mean(additions_percent_same))\n",
        "  return mean(percent_same_for_each_nda)\n",
        "\n",
        "print(\"Mean percent_same_for_each_nda: \", return_mean_percent_same_for_each_nda(multi_return_match_result))\n",
        "  "
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean percent_same_for_each_nda:  0.3022827958802241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-qwMIRwiB30"
      },
      "source": [
        "The above indicates that if three models are used, the top 10% of the results are shared by 30% of the models.  The following indicates if 2 models are using the results are shared by 55% of the two models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mks4GYowNKG5",
        "outputId": "09507bce-ef87-488f-ecaf-fde8d9fe46c8"
      },
      "source": [
        "scoring_method_list=[\n",
        "                     [scoring_method_bert, model_mpnet_base_v2, \"model_mpnet_base_v2\"],\n",
        "                     [scoring_method_bert,model_roberta_base_v2, \"model_roberta_base_v2\"],\n",
        "                     ]\n",
        "\n",
        "multi_return_match_result=[]\n",
        "for i in range(len(random_NDA_list)):\n",
        "  # print(f\"processing {str(i)} of {str(len(random_NDA_list))}\")\n",
        "  NDA=random_NDA_list[i]\n",
        "  multi_return_match_result.append(return_match(NDA, \"additions_with_context\", \"patents\", scoring_method_list, cutoff_percentage=.1))\n",
        "\n",
        "print(\"Mean percent_same_for_each_nda: \", return_mean_percent_same_for_each_nda(multi_return_match_result))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean percent_same_for_each_nda:  0.5502248546898962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWcqslfqhrEM"
      },
      "source": [
        "If a single model is use, the following should report 1, since all matches are the same to itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYfPng0qf1Zr",
        "outputId": "bc0b1cc5-bbf6-495f-973c-993ace321bfe"
      },
      "source": [
        "scoring_method_list=[\n",
        "                     [scoring_method_bert, model_mpnet_base_v2, \"model_mpnet_base_v2\"],\n",
        "                     ]\n",
        "\n",
        "multi_return_match_result=[]\n",
        "for i in range(len(random_NDA_list)):\n",
        "  # print(f\"processing {str(i)} of {str(len(random_NDA_list))}\")\n",
        "  NDA=random_NDA_list[i]\n",
        "  multi_return_match_result.append(return_match(NDA, \"additions_with_context\", \"patents\", scoring_method_list, cutoff_percentage=.1))\n",
        "\n",
        "print(\"Mean percent_same_for_each_nda: \", return_mean_percent_same_for_each_nda(multi_return_match_result))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean percent_same_for_each_nda:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv9TahFAip5L"
      },
      "source": [
        "If the cutoff percentage increases, then the number of matching claims should increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9rJ8aTqgoZ7",
        "outputId": "f8683db2-8c60-4848-ba57-55f5d8d524ed"
      },
      "source": [
        "scoring_method_list=[\n",
        "                      [scoring_method_bert, model_mpnet_base_v2, \"model_mpnet_base_v2\"],\n",
        "                     [scoring_method_bert,model_roberta_base_v2, \"model_roberta_base_v2\"],\n",
        "                     [scoring_method_spacy,_en_core_sci_lg_nlp, \"_en_core_sci_lg_nlp\"]\n",
        "                     ]\n",
        "\n",
        "multi_return_match_result=[]\n",
        "for i in range(len(random_NDA_list)):\n",
        "  # print(f\"processing {str(i)} of {str(len(random_NDA_list))}\")\n",
        "  NDA=random_NDA_list[i]\n",
        "  multi_return_match_result.append(return_match(NDA, \"additions_with_context\", \"patents\", scoring_method_list, cutoff_percentage=.2))\n",
        "\n",
        "print(\"Mean percent_same_for_each_nda: \", return_mean_percent_same_for_each_nda(multi_return_match_result))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean percent_same_for_each_nda:  0.38887973947123505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukjN1Ob5gl4M",
        "outputId": "a2c42a19-6632-49f0-e47c-7f9d987251ef"
      },
      "source": [
        "scoring_method_list=[\n",
        "                      [scoring_method_bert, model_mpnet_base_v2, \"model_mpnet_base_v2\"],\n",
        "                     [scoring_method_bert,model_roberta_base_v2, \"model_roberta_base_v2\"],\n",
        "                     [scoring_method_spacy,_en_core_sci_lg_nlp, \"_en_core_sci_lg_nlp\"]\n",
        "                     ]\n",
        "\n",
        "multi_return_match_result=[]\n",
        "for i in range(len(random_NDA_list)):\n",
        "  # print(f\"processing {str(i)} of {str(len(random_NDA_list))}\")\n",
        "  NDA=random_NDA_list[i]\n",
        "  multi_return_match_result.append(return_match(NDA, \"additions_with_context\", \"patents\", scoring_method_list, cutoff_percentage=.3))\n",
        "\n",
        "print(\"Mean percent_same_for_each_nda: \", return_mean_percent_same_for_each_nda(multi_return_match_result))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean percent_same_for_each_nda:  0.45128598488193755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTHOK619jKAC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}